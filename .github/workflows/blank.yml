name: Twitter API Scraper

on:
  workflow_dispatch:  # Allows manual trigger
  schedule:
    - cron: '0 0 * * *'  # Run daily at midnight

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    steps:
      - name: Check out the repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.8'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install tweepy

      - name: Run the scraper script
        env:
          API_KEY: ${{ secrets.TWITTER_API_KEY }}
          API_SECRET: ${{ secrets.TWITTER_API_SECRET }}
          ACCESS_TOKEN: ${{ secrets.TWITTER_ACCESS_TOKEN }}
          ACCESS_TOKEN_SECRET: ${{ secrets.TWITTER_ACCESS_TOKEN_SECRET }}
        run: |
          python -c "
          import tweepy
          api_key = '${{ secrets.TWITTER_API_KEY }}'
          api_secret = '${{ secrets.TWITTER_API_SECRET }}'
          access_token = '${{ secrets.TWITTER_ACCESS_TOKEN }}'
          access_token_secret = '${{ secrets.TWITTER_ACCESS_TOKEN_SECRET }}'
          
          # Authenticate with Twitter API
          auth = tweepy.OAuthHandler(api_key, api_secret)
          auth.set_access_token(access_token, access_token_secret)
          api = tweepy.API(auth)

          # Define keywords to search for Land NFTs
          keywords = ['#NFTLand', '#DigitalLand', '#NFTProperty']

          # Collect tweets
          def get_tweets(keyword, count=100):
              tweets = tweepy.Cursor(api.search_tweets, q=keyword, lang='en').items(count)
              return [{'text': tweet.text, 'user': tweet.user.screen_name, 'created_at': tweet.created_at} for tweet in tweets]

          # Fetch tweets with each keyword
          for keyword in keywords:
              tweets = get_tweets(keyword)
              print(f'Tweets for {keyword}:', tweets)
          "


# Print or save tweets
for tweet in all_tweets:
    print(tweet)
